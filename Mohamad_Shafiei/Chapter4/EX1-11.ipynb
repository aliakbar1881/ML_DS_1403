{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX1 : SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX2 : When using Gradient Descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX3 : The good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX4 :There is almost no difference after training: all these algorithms end up with very similar models and make predictions in exactly the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX5: This indicates that the modelhas started to overfit the training data. With early stopping you just stop training as soon as the validation error reaches the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX6 : With Stochastic and Mini-batch Gradient Descent, the curves are not so smooth, and it may be hard to know whether you have reached the minimum or not. One solution is to stop only after the validation error has been above the minimum for some time (when you are confident that the model will not do any better), then roll back the model parameters to the point where the validation error was at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX7 : SGD : سریعتر به نقطه بهینه میرسه چون توی هر مرحله یک نمونه بر میداره و  محاسباتش راحتره\n",
    "Batch GD : بیشتر تضمین میکنه که به نقطه بهینه  برسه چون همه داده هارو  بهش میدیم با پیاداری بیشتر و نرم تر به نقطه  بهینه میرسه اما محاسبات زیادی باید انجام بده و خیلی طول میکشه\n",
    "با تغییر لرنینگ ریت به  صورت داینامیک میشه کاری کرد که همگرا بشن و اینکه توی هر مرحله epoch داده ای که برای SGD یا mini برمیداریم قبلش شافل بزنیم و به ترتیب نباشن"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX8 :This means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model.\n",
    "راه حلش اینکه دیتا ترین رو بیشتر کنیم یا از رگیولاریزیشن مثلا l1 یا l2 استفاده کنیم یا فیچر هارو کمتر کنیم یا درجه مدل رو کمتر کنیم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX9 : بایاس زیاد چون داده ها رو نمیتونه یاد بگیره و مدل خیلی ساده و اندرفیت هست . مقدار آلفا  اگه زیاد بشه مدل ساده تر میشه چون ضرایب کمتر میکنه پس مقدار آلفا باید کم باشه  \n",
    "Irreducible error\n",
    "This part is due to the noisiness of the data itself. The only way to\n",
    "reduce this part of the error is to clean up the data (e.g., fix the data\n",
    "sources, such as broken sensors, or detect and remove outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX10 :\n",
    "1 :It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression.\n",
    "درکل با کمی منظم سازی جلوگیری میشه از اینکه ضرایب داده ها خیلی زیاد بشن  و مدل اورفیت بشه  و اگه داده ها نویز دارن با این کار مدل عملکرد بهتری پیدا میکنه و پایدار تر میشه\n",
    "\n",
    "2:if you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features’ weights down to zero\n",
    "وقتی  میبینیم بعضی از فیچر هامون یوزلس هستن از لسو استفاده میکنیم چون با این کار  ویژگی های کم اهمیت رو حذف  میکنه \n",
    "\n",
    "3:Elastic Net is preferred over Lasso because Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated\n",
    "وقتی تعداد ویژگی ها بیشتر از تعداد نمونه ها باشه و  یا اینکه ویژگی ها کورولیشن باهم داشته باشن ممکنه لسو بد عمل کنه ( چون لسو میخواد بعضی از ویژگی هارو کلا از بین ببره) پس بهتره اینجور مواقع از الستیک استفاده کنیم که یه بالانسی برقرار بشه"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX11 : به  نظرم دوتا لجستیک باشه بهتره چون سافت مکس از بین چند کلاس یک خروجی میده و میگه مال کدوم  یکی هست اما اینجا دوتا روز وشب ، فضا باز و بیرون مستقل  از هم دیگه هستن و هر کدوم  با یک لجستیک باشه منطقی تره"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
